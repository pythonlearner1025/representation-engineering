{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "988c3e22-f7d3-4665-9087-cac13a68fc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/workspace/representation-engineering/examples/honesty', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/dist-packages', '/workspace/transformers/src', '/workspace/representation-engineering', '/workspace/repeng', '/usr/lib/python3/dist-packages', '/workspace/transformers']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/workspace/transformers')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "180b0dc9-4fca-4931-b746-d2ee0e88bdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b39f1baa-ca8c-4951-9704-d6c4ab45b2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig, pipeline, AutoModelForCausalLM\n",
    "from transformers.generation.streamers import TextStreamer\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "\n",
    "from repe.rep_control_contrast_vec import ContrastVecLlamaForCausalLM, ContrastVecMistralForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59d81fc0-5bd2-440d-8767-5c5d5cf6686d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "from queue import Queue\n",
    "\n",
    "class Streamer(TextStreamer):\n",
    "    def __init__(\n",
    "        self, tokenizer: \"AutoTokenizer\", total=128, skip_prompt: bool = False, timeout: Optional[float] = None, **decode_kwargs\n",
    "    ):\n",
    "        super().__init__(tokenizer, skip_prompt, **decode_kwargs)\n",
    "        self.text_queue = Queue()\n",
    "        self.stop_signal = None\n",
    "        self.timeout = timeout\n",
    "        self.repe = None\n",
    "        self.c = 0\n",
    "        self.total = 128\n",
    "\n",
    "    def on_finalized_text(self, text: str, stream_end: bool = False, corrs=None):\n",
    "        \"\"\"Put the new text in the queue. If the stream is ending, also put a stop signal in the queue.\"\"\"\n",
    "        self.text_queue.put((text,corrs), timeout=self.timeout)\n",
    "        if stream_end:\n",
    "            self.text_queue.put((self.stop_signal,None), timeout=self.timeout)\n",
    "        new_repe = {\n",
    "            \"dynamic_compute\": True,\n",
    "            \"control_layers\": list(range(10, 20)),\n",
    "            \"repe_vecs\": get_random_repe_vecs(self.c, self.total),\n",
    "            \"compute_corr\": get_corr(),\n",
    "            \"pos_inputs\": ,\n",
    "            \"neg_inputs\":\n",
    "        }\n",
    "        self.set_repe(new_repe)\n",
    "        super().on_finalized_text(text)\n",
    "        self.c += 1\n",
    "        if corrs is not None:\n",
    "            #print(corrs.cpu())\n",
    "            pass\n",
    "        \n",
    "    def set_repe(self, repe):\n",
    "        self.repe = repe\n",
    "    \n",
    "    def get_repe(self):\n",
    "        return self.repe\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    # returned is gonna be Tuple (tok, corr)\n",
    "    def __next__(self):\n",
    "        value = self.text_queue.get(timeout=self.timeout)\n",
    "        if value == self.stop_signal:\n",
    "            raise StopIteration()\n",
    "        else:\n",
    "            return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b9c7545-d846-4cb2-9945-1c6d9313007c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eaf6412ea6e4468a942683635b5a19e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM\n",
    "\n",
    "name = \"meta-llama/Meta-Llama-3-8b-Instruct\"\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    name, \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map='cuda'\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "\n",
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8432e57-625a-4034-a0bb-6e8e6d1f85fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "temple = '''<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a conversational assistant. Egange in long engaging conversation with the user. <|eot_id|><|start_header_id|>user<|end_header_id|>{prompt}<|start_header_id|>assistant<|end_header_id|>'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5889ce40-7925-4f9f-911e-19722e13846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import random\n",
    "import torch\n",
    "\n",
    "NUM_TOKEN = 512  # Example number, set as required\n",
    "vec1 = torch.load('../data/anger_rep_reader.pt')\n",
    "vec2 = torch.load('../data/happiness_rep_reader.pt')\n",
    "\n",
    "def load_vec(vec):\n",
    "    for k,v in vec['dir'].items():\n",
    "        vec['dir'][k] = torch.tensor(v).float().cuda()\n",
    "        s = vec['signs'][k]\n",
    "        vec['signs'][k] = torch.tensor(s).float().cuda()\n",
    "        \n",
    "load_vec(vec1)\n",
    "load_vec(vec2)\n",
    "\n",
    "def get_corr():\n",
    "    return [\n",
    "        vec1,\n",
    "        vec2\n",
    "    ]\n",
    "\n",
    "def get_random_repe_vecs(c, total): \n",
    "   # print(f'({c/total})', end='')\n",
    "    # tuple(vec1, 0)\n",
    "    # vec1['dir'][10] = dir\n",
    "    # vec1['signs'][10] = sign \n",
    "    return {\"a\": (vec1, -1.8), 'b': (vec2, 0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d64f2794-e8c4-41af-8d70-69feef9fd7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin pipeline generation...\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a conversational assistant. Egange in long engaging conversation with the user. <|eot_id|><|start_header_id|>user<|end_header_id|>What are human beings like?<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "What a complex and intriguing question! Human beings are a puzzle, a paradox, a riddle wrapped in a mystery. I'm not sure I can provide a definitive answer, but I'll try to explore the many facets of humanity with you.\n",
      "\n",
      "I think it's important to acknowledge that human beings are incredibly diverse, with no two individuals being exactly alike. We're a species that's capable of both good and evil, of kindness and cruelty, of confusion and uncertainty. We're flawed, imperfect, and often struggling to make sense of the world around us.\n",
      "\n",
      "One of the most challenging aspects of humanity is our tendency to be contradictory. We're capable of great cruelty, but we're also capable of great empathy and compassion. We're prone to making mistakes, but we're also capable of learning from those mistakes and trying to do better. It's a delicate balance, and I'm not sure there's a single answer to what's \"wrong\" with humanity.\n",
      "\n",
      "I think it's also important to acknowledge that human beings are not just individuals, but also a part of a larger, interconnected web of relationships. We're not just isolated, autonomous beings, but rather we're all connected, and our actions and decisions have consequences that ripple out and affect others.\n",
      "\n",
      "I'm not sure I can provide a definitive answer to what's \"wrong\" with humanity, but I think it's important to recognize that we're all struggling to figure things out, to make sense of the world, and to find our place in it. We're all in this together, and I'm not sure there's a single answer to what's \"wrong\" with humanity.\n",
      "\n",
      "I'm not sure I'm going to be able to provide a definitive answer to your question, but I'm not sure I'm going to be able to provide a definitive answer to anything. I'm not sure I'm going to be able to provide a definitive answer to anything.<|eot_id|>generation end...\n"
     ]
    }
   ],
   "source": [
    "streamer = Streamer(tokenizer, total=NUM_TOKEN)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=NUM_TOKEN,\n",
    "    streamer=streamer,\n",
    "    do_sample=False,\n",
    "    use_cache=False\n",
    ")\n",
    "\n",
    "prompt = \"What are human beings like?\"\n",
    "ans = pipe(temple.format(prompt=prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6261d5d9-b405-44b3-91d3-1131489bed97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
